{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Spam or Ham ?  \n",
    "\n",
    "In this notebook, I will use the sms data from `spam-sms.csv` file in order to predict whether or not a sms is a spam or a ham. \n",
    "\n",
    "I implemented below several classification algorithm :\n",
    "- Naive Bayes algorithms (Gaussian, Bernoulli and Multinomial Naive Bayes)\n",
    "- Logistic Regression\n",
    "- tree algorithm (Cart, ID3)\n",
    "- ensemble method (Bagging, AdaBoost, Random Forest)\n",
    "with sklearn.\n",
    "\n",
    "Julien Verdun\n",
    "07/12/2020"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from collections import Counter \n",
    "import operator\n",
    "import re\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score, precision_score, roc_auc_score\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import cross_validate, KFold\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn import tree\n",
    "import time \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  target                                                sms\n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>sms</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "spam_df = pd.read_csv(\"spam-sms.csv\",header=0, encoding='latin-1',names=[\"target\",\"sms\",\"1\",\"2\",\"3\"])[[\"target\",\"sms\"]]\n",
    "spam_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ham     4825\nspam     747\nName: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "value_counts = spam_df[\"target\"].value_counts()\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ham     1494\n",
       "spam     747\n",
       "Name: target, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "spam_df = pd.concat([spam_df[spam_df[\"target\"]==\"ham\"].sample(frac=2*value_counts[1]/value_counts[0],random_state=200),spam_df[spam_df[\"target\"]==\"spam\"]])\n",
    "\n",
    "spam_df[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ham     1494\n",
       "spam     747\n",
       "Name: target, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "spam_df[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "input_data = vectorizer.fit_transform(spam_df[\"sms\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bintarget(x):\n",
    "    if x == \"ham\":\n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     target                                                sms  \\\n",
       "1840    ham  Yeah. I got a list with only u and Joanna if I...   \n",
       "3067    ham             Boy you best get yo ass out here quick   \n",
       "4720    ham         Yup. Anything lor, if u dun wan it's ok...   \n",
       "1473    ham  Will do, you gonna be at blake's all night? I ...   \n",
       "4573    ham                           :( but your not here....   \n",
       "\n",
       "                                         vectorized_sms  targetbin  \n",
       "1840  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...          0  \n",
       "3067  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...          0  \n",
       "4720  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...          0  \n",
       "1473  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...          0  \n",
       "4573  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...          0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>sms</th>\n      <th>vectorized_sms</th>\n      <th>targetbin</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1840</th>\n      <td>ham</td>\n      <td>Yeah. I got a list with only u and Joanna if I...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3067</th>\n      <td>ham</td>\n      <td>Boy you best get yo ass out here quick</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4720</th>\n      <td>ham</td>\n      <td>Yup. Anything lor, if u dun wan it's ok...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1473</th>\n      <td>ham</td>\n      <td>Will do, you gonna be at blake's all night? I ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4573</th>\n      <td>ham</td>\n      <td>:( but your not here....</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "spam_df[\"vectorized_sms\"] = [list(elt) for elt in list(input_data)]\n",
    "\n",
    "spam_df[\"targetbin\"] = spam_df[\"target\"].apply(lambda x : bintarget(x))\n",
    "\n",
    "spam_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = np.array(spam_df[\"targetbin\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = {\n",
    "'GNB': GaussianNB(),\n",
    "'MNG': MultinomialNB(), \n",
    "'BNB' : BernoulliNB(),\n",
    "'LOG' : LogisticRegression(random_state=1),\n",
    "'CART' : tree.DecisionTreeClassifier(random_state=1,criterion='gini'),\n",
    "'ID3' : tree.DecisionTreeClassifier(random_state=1,criterion='entropy'),\n",
    "'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "'RF': RandomForestClassifier(n_estimators=50, random_state=1),\n",
    "'BGC': BaggingClassifier(n_estimators=50),\n",
    "'ADB': AdaBoostClassifier(n_estimators=50)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=============== GNB ============== \n",
      "\n",
      "Accuracy : 0.876 +/- 0.021\n",
      "AUC : 0.897 +/- 0.020\n",
      "Precision : 0.744 +/- 0.031\n",
      "Recall : 0.961 +/- 0.022\n",
      "F1 : 0.838 +/- 0.026\n",
      "Total : 6.857\n",
      "\n",
      "\n",
      "=============== MNG ============== \n",
      "\n",
      "Accuracy : 0.972 +/- 0.010\n",
      "AUC : 0.984 +/- 0.008\n",
      "Precision : 0.967 +/- 0.015\n",
      "Recall : 0.947 +/- 0.025\n",
      "F1 : 0.957 +/- 0.017\n",
      "Total : 4.266\n",
      "\n",
      "\n",
      "=============== BNB ============== \n",
      "\n",
      "Accuracy : 0.963 +/- 0.012\n",
      "AUC : 0.995 +/- 0.005\n",
      "Precision : 0.991 +/- 0.013\n",
      "Recall : 0.897 +/- 0.037\n",
      "F1 : 0.941 +/- 0.021\n",
      "Total : 5.867\n",
      "\n",
      "\n",
      "=============== LOG ============== \n",
      "\n",
      "Accuracy : 0.963 +/- 0.013\n",
      "AUC : 0.987 +/- 0.007\n",
      "Precision : 0.971 +/- 0.013\n",
      "Recall : 0.916 +/- 0.035\n",
      "F1 : 0.942 +/- 0.023\n",
      "Total : 13.938\n",
      "\n",
      "\n",
      "=============== CART ============== \n",
      "\n",
      "Accuracy : 0.944 +/- 0.018\n",
      "AUC : 0.933 +/- 0.024\n",
      "Precision : 0.927 +/- 0.025\n",
      "Recall : 0.902 +/- 0.045\n",
      "F1 : 0.914 +/- 0.030\n",
      "Total : 72.879\n",
      "\n",
      "\n",
      "=============== ID3 ============== \n",
      "\n",
      "Accuracy : 0.932 +/- 0.027\n",
      "AUC : 0.918 +/- 0.029\n",
      "Precision : 0.913 +/- 0.053\n",
      "Recall : 0.877 +/- 0.037\n",
      "F1 : 0.895 +/- 0.042\n",
      "Total : 44.321\n",
      "\n",
      "\n",
      "=============== KNN ============== \n",
      "\n",
      "Accuracy : 0.829 +/- 0.023\n",
      "AUC : 0.896 +/- 0.030\n",
      "Precision : 1.000 +/- 0.000\n",
      "Recall : 0.484 +/- 0.076\n",
      "F1 : 0.649 +/- 0.072\n",
      "Total : 156.716\n",
      "\n",
      "\n",
      "=============== RF ============== \n",
      "\n",
      "Accuracy : 0.957 +/- 0.008\n",
      "AUC : 0.989 +/- 0.009\n",
      "Precision : 0.996 +/- 0.007\n",
      "Recall : 0.875 +/- 0.028\n",
      "F1 : 0.931 +/- 0.016\n",
      "Total : 60.097\n",
      "\n",
      "\n",
      "=============== BGC ============== \n",
      "\n",
      "Accuracy : 0.948 +/- 0.018\n",
      "AUC : 0.978 +/- 0.009\n",
      "Precision : 0.936 +/- 0.030\n",
      "Recall : 0.905 +/- 0.040\n",
      "F1 : 0.920 +/- 0.029\n",
      "Total : 590.669\n",
      "\n",
      "\n",
      "=============== ADB ============== \n",
      "\n",
      "Accuracy : 0.941 +/- 0.019\n",
      "AUC : 0.972 +/- 0.013\n",
      "Precision : 0.946 +/- 0.029\n",
      "Recall : 0.872 +/- 0.035\n",
      "F1 : 0.908 +/- 0.029\n",
      "Total : 327.668\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def run_classifiers(clfs, X, Y):\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "    for i in clfs:\n",
    "        print(\"=============== {0} ============== \\n\".format(i))\n",
    "        t0 = time.time()\n",
    "        clf = clfs[i]\n",
    "        cv_acc = cross_validate(clf, X,Y, cv=kf,scoring=['roc_auc','precision','recall','accuracy','f1'])\n",
    "        print(\"Accuracy : {1:.3f} +/- {2:.3f}\".format(i, np.mean(cv_acc['test_accuracy']), np.std(cv_acc['test_accuracy'])))\n",
    "        print(\"AUC : {1:.3f} +/- {2:.3f}\".format(i, np.mean(cv_acc['test_roc_auc']), np.std(cv_acc['test_roc_auc'])))\n",
    "        print(\"Precision : {1:.3f} +/- {2:.3f}\".format(i, np.mean(cv_acc['test_precision']), np.std(cv_acc['test_precision'])))\n",
    "        print(\"Recall : {1:.3f} +/- {2:.3f}\".format(i, np.mean(cv_acc['test_recall']), np.std(cv_acc['test_recall'])))\n",
    "        print(\"F1 : {1:.3f} +/- {2:.3f}\".format(i, np.mean(cv_acc['test_f1']), np.std(cv_acc['test_f1'])))\n",
    "        print(\"Total : {1:.3f}\".format(i,time.time()-t0))\n",
    "        print()\n",
    "        print()\n",
    "\n",
    "run_classifiers(clfs,input_data, output_data)"
   ]
  },
  {
   "source": [
    "All the different classifiers reached high performances (gretter than 80% accuracy). Some algorithm are slower than the other, especially the tree classifiers (they take time with a lot of input data).\n",
    "\n",
    "The **Naive Bayes** methods give better results than the method I implemented from scratch. In fact, one major difference is the word vectorizer which is optimize in sklearn. In my vectorizer I didn't take into account the words plurial and so on. \n",
    "\n",
    "\n",
    "The best method is the **Multinomial Naive Bayes** method with an accuracy of 97.2%, a F-measure of 95.7%,and a training time relatively low (less than 5 seconds)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}